{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n",
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_last</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Walker</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Palmer</td>\n",
       "      <td>Alton</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mc Cleod</td>\n",
       "      <td>Alicia</td>\n",
       "      <td>nh_black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scarborough</td>\n",
       "      <td>Dale</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Walker</td>\n",
       "      <td>Daniel</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653889</th>\n",
       "      <td>Walters</td>\n",
       "      <td>William</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653890</th>\n",
       "      <td>Sawyer</td>\n",
       "      <td>Matthew</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653891</th>\n",
       "      <td>Thomas</td>\n",
       "      <td>Janine</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653892</th>\n",
       "      <td>Campbell</td>\n",
       "      <td>Angel</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653893</th>\n",
       "      <td>Bruner</td>\n",
       "      <td>Jeb</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13653894 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name_first  name_last      race\n",
       "0              Walker  Elizabeth  nh_white\n",
       "1              Palmer      Alton  nh_white\n",
       "2            Mc Cleod     Alicia  nh_black\n",
       "3         Scarborough       Dale  nh_white\n",
       "4              Walker     Daniel  nh_white\n",
       "...               ...        ...       ...\n",
       "13653889      Walters    William  nh_white\n",
       "13653890       Sawyer    Matthew  nh_white\n",
       "13653891       Thomas     Janine  nh_white\n",
       "13653892     Campbell      Angel     other\n",
       "13653893       Bruner        Jeb  nh_white\n",
       "\n",
       "[13653894 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "SAMPLE = 1000000\n",
    "EPOCHS = 20\n",
    "\n",
    "# Florida voter\n",
    "df = pd.read_csv('../dataverse_files/fl_voter_reg/fl_reg_name_race.csv.gz')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "df['race'] = df.race.map({'native_indian': 'other', 'asian': 'asian', 'nh_black': 'nh_black', 'hispanic': 'hispanic', 'nh_white': 'nh_white', 'other': 'other', 'multi_racial': 'other', 'unknown': 'unknown'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_last</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>779856</th>\n",
       "      <td>Kelley</td>\n",
       "      <td>Mi</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10722411</th>\n",
       "      <td>Pitman</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9871739</th>\n",
       "      <td>Patel</td>\n",
       "      <td>Ranjanbala</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6278811</th>\n",
       "      <td>Inch</td>\n",
       "      <td>Corazon</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6085057</th>\n",
       "      <td>Mathew</td>\n",
       "      <td>Lilly</td>\n",
       "      <td>asian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2308712</th>\n",
       "      <td>Castro</td>\n",
       "      <td>Gisele</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5945501</th>\n",
       "      <td>Brooks</td>\n",
       "      <td>Dyron</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160933</th>\n",
       "      <td>Carson</td>\n",
       "      <td>Mary</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397639</th>\n",
       "      <td>Amer</td>\n",
       "      <td>Ossama</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300768</th>\n",
       "      <td>Saleh</td>\n",
       "      <td>Rania</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name_first   name_last   race\n",
       "779856       Kelley          Mi  asian\n",
       "10722411     Pitman       Ariel  asian\n",
       "9871739       Patel  Ranjanbala  asian\n",
       "6278811        Inch     Corazon  asian\n",
       "6085057      Mathew       Lilly  asian\n",
       "...             ...         ...    ...\n",
       "2308712      Castro      Gisele  other\n",
       "5945501      Brooks       Dyron  other\n",
       "8160933      Carson        Mary  other\n",
       "8397639        Amer      Ossama  other\n",
       "300768        Saleh       Rania  other\n",
       "\n",
       "[800000 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf = df[df.race.isin(['unknown']) == False].groupby(['race']).sample(int(SAMPLE/5), random_state=21)\n",
    "del df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_first'] = sdf.name_first.str.title()\n",
    "sdf['name_last'] = sdf.name_last.str.title()\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian        200000\n",
       "nh_black     200000\n",
       "nh_white     200000\n",
       "other        200000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = sdf.groupby('race').agg({'name_last': 'count'})\n",
    "rdf.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_name_five_cat_race.csv', columns=[])\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>49116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>43038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>12241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>35977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian         49116\n",
       "nh_black      43038\n",
       "nh_white      12241\n",
       "other         35977"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.groupby('race').agg({'name_last': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 1313\n",
      "Max feature len = 44, Avg. feature len = 12\n"
     ]
    }
   ],
   "source": [
    "# concat last name and first name\n",
    "sdf['name_last_name_first'] = sdf['name_last'] + ' ' + sdf['name_first']\n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640000 train sequences\n",
      "160000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (640000, 20)\n",
      "X_test shape: (160000, 20)\n",
      "4 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (640000, 4)\n",
      "y_test shape: (160000, 4)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 20 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 20, 32)            42016     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 124,964\n",
      "Trainable params: 124,964\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "input_ = keras.layers.Input(shape=(feature_len))\n",
    "layer_ = Embedding(num_words, 32) (input_)\n",
    "layer_ = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(layer_, training=True)\n",
    "output = Dense(num_classes, activation='softmax') (layer_)\n",
    "\n",
    "model = keras.models.Model(inputs=input_, outputs=output)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/20\n",
      "18000/18000 [==============================] - 393s 20ms/step - loss: 1.1300 - accuracy: 0.4909 - val_loss: 1.0553 - val_accuracy: 0.5357\n",
      "Epoch 2/20\n",
      "18000/18000 [==============================] - 376s 21ms/step - loss: 1.0413 - accuracy: 0.5462 - val_loss: 1.0253 - val_accuracy: 0.5538\n",
      "Epoch 3/20\n",
      "18000/18000 [==============================] - 345s 19ms/step - loss: 1.0134 - accuracy: 0.5613 - val_loss: 1.0120 - val_accuracy: 0.5621\n",
      "Epoch 4/20\n",
      "18000/18000 [==============================] - 338s 19ms/step - loss: 0.9968 - accuracy: 0.5697 - val_loss: 1.0029 - val_accuracy: 0.5675\n",
      "Epoch 5/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9871 - accuracy: 0.5749 - val_loss: 0.9989 - val_accuracy: 0.5699\n",
      "Epoch 6/20\n",
      "18000/18000 [==============================] - 332s 18ms/step - loss: 0.9802 - accuracy: 0.5806 - val_loss: 0.9943 - val_accuracy: 0.5716\n",
      "Epoch 7/20\n",
      "18000/18000 [==============================] - 332s 18ms/step - loss: 0.9748 - accuracy: 0.5823 - val_loss: 0.9905 - val_accuracy: 0.5760\n",
      "Epoch 8/20\n",
      "18000/18000 [==============================] - 337s 19ms/step - loss: 0.9714 - accuracy: 0.5838 - val_loss: 0.9909 - val_accuracy: 0.5734\n",
      "Epoch 9/20\n",
      "18000/18000 [==============================] - 350s 19ms/step - loss: 0.9678 - accuracy: 0.5853 - val_loss: 0.9887 - val_accuracy: 0.5770\n",
      "Epoch 10/20\n",
      "18000/18000 [==============================] - 336s 19ms/step - loss: 0.9671 - accuracy: 0.5859 - val_loss: 0.9876 - val_accuracy: 0.5763\n",
      "Epoch 11/20\n",
      "18000/18000 [==============================] - 332s 18ms/step - loss: 0.9650 - accuracy: 0.5881 - val_loss: 0.9863 - val_accuracy: 0.5781\n",
      "Epoch 12/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9609 - accuracy: 0.5903 - val_loss: 0.9883 - val_accuracy: 0.5755\n",
      "Epoch 13/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9598 - accuracy: 0.5896 - val_loss: 0.9848 - val_accuracy: 0.5769\n",
      "Epoch 14/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9581 - accuracy: 0.5911 - val_loss: 0.9841 - val_accuracy: 0.5787\n",
      "Epoch 15/20\n",
      "18000/18000 [==============================] - 336s 19ms/step - loss: 0.9568 - accuracy: 0.5924 - val_loss: 0.9830 - val_accuracy: 0.5793\n",
      "Epoch 16/20\n",
      "18000/18000 [==============================] - 333s 19ms/step - loss: 0.9576 - accuracy: 0.5905 - val_loss: 0.9836 - val_accuracy: 0.5782\n",
      "Epoch 17/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9564 - accuracy: 0.5911 - val_loss: 0.9838 - val_accuracy: 0.5790\n",
      "Epoch 18/20\n",
      "18000/18000 [==============================] - 333s 18ms/step - loss: 0.9551 - accuracy: 0.5921 - val_loss: 0.9851 - val_accuracy: 0.5776\n",
      "Epoch 19/20\n",
      "18000/18000 [==============================] - 342s 19ms/step - loss: 0.9533 - accuracy: 0.5930 - val_loss: 0.9830 - val_accuracy: 0.5797\n",
      "Epoch 20/20\n",
      "18000/18000 [==============================] - 335s 19ms/step - loss: 0.9531 - accuracy: 0.5939 - val_loss: 0.9852 - val_accuracy: 0.5793\n",
      "5000/5000 [==============================] - 25s 5ms/step - loss: 0.9845 - accuracy: 0.5787\n",
      "Test score: 0.9844765663146973\n",
      "Test accuracy: 0.5787374973297119\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.9844765663146973\n",
      "Test accuracy: 0.5787374973297119\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 - 23s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       asian       0.74      0.70      0.72     40000\n",
      "    nh_black       0.59      0.72      0.64     40000\n",
      "    nh_white       0.51      0.70      0.59     40000\n",
      "       other       0.45      0.21      0.28     40000\n",
      "\n",
      "    accuracy                           0.58    160000\n",
      "   macro avg       0.57      0.58      0.56    160000\n",
      "weighted avg       0.57      0.58      0.56    160000\n",
      "\n",
      "[[27887  2832  4627  4654]\n",
      " [ 1071 28673  8161  2095]\n",
      " [ 1462  7264 27815  3459]\n",
      " [ 7323 10176 14266  8235]]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test, verbose=2) # to predict probability\n",
    "y_pred = np.argmax(p, axis=-1)\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/fl_voter_reg/lstm/fl_all_fullname_lstm_5_cat.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_all_fullname_vocab_5_cat.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
