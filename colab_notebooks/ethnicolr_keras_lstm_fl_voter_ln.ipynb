{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n",
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_first</th>\n",
       "      <th>name_last</th>\n",
       "      <th>race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841323</th>\n",
       "      <td>Torres</td>\n",
       "      <td>Jose</td>\n",
       "      <td>hispanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408926</th>\n",
       "      <td>Da Silva</td>\n",
       "      <td>Amanda</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733118</th>\n",
       "      <td>Mc Ghee</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13104513</th>\n",
       "      <td>KARAM</td>\n",
       "      <td>Melinda</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9156114</th>\n",
       "      <td>BREWER</td>\n",
       "      <td>Laia</td>\n",
       "      <td>nh_black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076722</th>\n",
       "      <td>Antunez Avila</td>\n",
       "      <td>Robert</td>\n",
       "      <td>hispanc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10023679</th>\n",
       "      <td>DAVIS</td>\n",
       "      <td>Wyatt</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5846252</th>\n",
       "      <td>Scott</td>\n",
       "      <td>Jacquelyn</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5959131</th>\n",
       "      <td>Parton</td>\n",
       "      <td>Douglas</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12031764</th>\n",
       "      <td>Knapp</td>\n",
       "      <td>Liza</td>\n",
       "      <td>nh_white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name_first  name_last      race\n",
       "841323           Torres       Jose   hispanc\n",
       "1408926        Da Silva     Amanda  nh_white\n",
       "1733118         Mc Ghee     Sandra  nh_white\n",
       "13104513          KARAM    Melinda  nh_white\n",
       "9156114          BREWER       Laia  nh_black\n",
       "...                 ...        ...       ...\n",
       "3076722   Antunez Avila     Robert   hispanc\n",
       "10023679          DAVIS      Wyatt  nh_white\n",
       "5846252           Scott  Jacquelyn  nh_white\n",
       "5959131          Parton    Douglas  nh_white\n",
       "12031764          Knapp       Liza  nh_white\n",
       "\n",
       "[1000000 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "NGRAMS = 2\n",
    "SAMPLE = 1000000\n",
    "EPOCHS = 15\n",
    "\n",
    "# Florida voter\n",
    "df = pd.read_csv('../dataverse_files/fl_reg_name_race.csv.gz')\n",
    "df.dropna(subset=['name_first', 'name_last'], inplace=True)\n",
    "sdf = df[df.race.isin(['multi_racial', 'native_indian', 'other', 'unknown']) == False].sample(SAMPLE, random_state=21)\n",
    "del df\n",
    "\n",
    "# Additional features\n",
    "sdf['name_last'] = sdf.name_last.str.title()\n",
    "\n",
    "sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>19431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanc</th>\n",
       "      <td>166865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>142675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>671029</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian         19431\n",
       "hispanc      166865\n",
       "nh_black     142675\n",
       "nh_white     671029"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = sdf.groupby('race').agg({'name_last': 'count'})\n",
    "rdf.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_ln_race.csv', columns=[])\n",
    "rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_last</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>asian</th>\n",
       "      <td>8933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hispanc</th>\n",
       "      <td>17512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_black</th>\n",
       "      <td>33422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nh_white</th>\n",
       "      <td>25575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name_last\n",
       "race               \n",
       "asian          8933\n",
       "hispanc       17512\n",
       "nh_black      33422\n",
       "nh_white      25575"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf.groupby('race').agg({'name_last': 'nunique'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_words = 1103\n",
      "Max feature len = 25, Avg. feature len = 5\n"
     ]
    }
   ],
   "source": [
    "# last name only\n",
    "sdf['name_last_name_first'] = sdf['name_last']\n",
    "\n",
    "# build n-gram list\n",
    "vect = CountVectorizer(analyzer='char', max_df=0.3, min_df=3, ngram_range=(NGRAMS, NGRAMS), lowercase=False) \n",
    "a = vect.fit_transform(sdf.name_last_name_first)\n",
    "vocab = vect.vocabulary_\n",
    "\n",
    "# sort n-gram by freq (highest -> lowest)\n",
    "words = []\n",
    "for b in vocab:\n",
    "    c = vocab[b]\n",
    "    #print(b, c, a[:, c].sum())\n",
    "    words.append((a[:, c].sum(), b))\n",
    "    #break\n",
    "words = sorted(words, reverse=True)\n",
    "words_list = [w[1] for w in words]\n",
    "num_words = len(words_list)\n",
    "print(\"num_words = %d\" % num_words)\n",
    "\n",
    "\n",
    "def find_ngrams(text, n):\n",
    "    a = zip(*[text[i:] for i in range(n)])\n",
    "    wi = []\n",
    "    for i in a:\n",
    "        w = ''.join(i)\n",
    "        try:\n",
    "            idx = words_list.index(w)\n",
    "        except:\n",
    "            idx = 0\n",
    "        wi.append(idx)\n",
    "    return wi\n",
    "\n",
    "# build X from index of n-gram sequence\n",
    "X = np.array(sdf.name_last_name_first.apply(lambda c: find_ngrams(c, NGRAMS)))\n",
    "\n",
    "# check max/avg feature\n",
    "X_len = []\n",
    "for x in X:\n",
    "    X_len.append(len(x))\n",
    "\n",
    "max_feature_len = max(X_len)\n",
    "avg_feature_len = int(np.mean(X_len))\n",
    "\n",
    "print(\"Max feature len = %d, Avg. feature len = %d\" % (max_feature_len, avg_feature_len))\n",
    "y = np.array(sdf.race.astype('category').cat.codes)\n",
    "\n",
    "# Split train and test dataset\n",
    "X_train,  X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a LSTM model\n",
    "\n",
    "ref: http://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 train sequences\n",
      "200000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (800000, 20)\n",
      "X_test shape: (200000, 20)\n",
      "4 classes\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (800000, 4)\n",
      "y_test shape: (200000, 4)\n"
     ]
    }
   ],
   "source": [
    "'''The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "Notes:\n",
    "\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "max_features = num_words # 20000\n",
    "feature_len = 20 # avg_feature_len # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=feature_len)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=feature_len)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "print(num_classes, 'classes')\n",
    "\n",
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 32)            35296     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 118,244\n",
      "Trainable params: 118,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 32, input_length=feature_len))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/15\n",
      "22500/22500 [==============================] - 467s 21ms/step - loss: 0.7483 - accuracy: 0.7353 - val_loss: 0.6705 - val_accuracy: 0.7651\n",
      "Epoch 2/15\n",
      "22500/22500 [==============================] - 420s 19ms/step - loss: 0.6692 - accuracy: 0.7664 - val_loss: 0.6494 - val_accuracy: 0.7740\n",
      "Epoch 3/15\n",
      "22500/22500 [==============================] - 432s 19ms/step - loss: 0.6485 - accuracy: 0.7753 - val_loss: 0.6435 - val_accuracy: 0.7769\n",
      "Epoch 4/15\n",
      "22500/22500 [==============================] - 429s 19ms/step - loss: 0.6418 - accuracy: 0.7773 - val_loss: 0.6352 - val_accuracy: 0.7806\n",
      "Epoch 5/15\n",
      "22500/22500 [==============================] - 418s 19ms/step - loss: 0.6351 - accuracy: 0.7800 - val_loss: 0.6327 - val_accuracy: 0.7819\n",
      "Epoch 6/15\n",
      "22500/22500 [==============================] - 427s 19ms/step - loss: 0.6332 - accuracy: 0.7811 - val_loss: 0.6308 - val_accuracy: 0.7825\n",
      "Epoch 7/15\n",
      "22500/22500 [==============================] - 449s 20ms/step - loss: 0.6281 - accuracy: 0.7831 - val_loss: 0.6309 - val_accuracy: 0.7824\n",
      "Epoch 8/15\n",
      "22500/22500 [==============================] - 431s 19ms/step - loss: 0.6295 - accuracy: 0.7826 - val_loss: 0.6285 - val_accuracy: 0.7840\n",
      "Epoch 9/15\n",
      "22500/22500 [==============================] - 445s 20ms/step - loss: 0.6276 - accuracy: 0.7836 - val_loss: 0.6284 - val_accuracy: 0.7841\n",
      "Epoch 10/15\n",
      "22500/22500 [==============================] - 456s 20ms/step - loss: 0.6250 - accuracy: 0.7847 - val_loss: 0.6285 - val_accuracy: 0.7859\n",
      "Epoch 11/15\n",
      "22500/22500 [==============================] - 447s 20ms/step - loss: 0.6246 - accuracy: 0.7847 - val_loss: 0.6271 - val_accuracy: 0.7853\n",
      "Epoch 12/15\n",
      "22500/22500 [==============================] - 432s 19ms/step - loss: 0.6254 - accuracy: 0.7837 - val_loss: 0.6268 - val_accuracy: 0.7848\n",
      "Epoch 13/15\n",
      "22500/22500 [==============================] - 415s 18ms/step - loss: 0.6249 - accuracy: 0.7848 - val_loss: 0.6266 - val_accuracy: 0.7853\n",
      "Epoch 14/15\n",
      "22500/22500 [==============================] - 414s 18ms/step - loss: 0.6227 - accuracy: 0.7855 - val_loss: 0.6275 - val_accuracy: 0.7855\n",
      "Epoch 15/15\n",
      "22500/22500 [==============================] - 416s 18ms/step - loss: 0.6222 - accuracy: 0.7853 - val_loss: 0.6255 - val_accuracy: 0.7860\n",
      "6250/6250 [==============================] - 26s 4ms/step - loss: 0.6247 - accuracy: 0.7855\n",
      "Test score: 0.6247256398200989\n",
      "Test accuracy: 0.7854949831962585\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6247256398200989\n",
      "Test accuracy: 0.7854949831962585\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 22s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       asian       0.71      0.30      0.42      3886\n",
      "     hispanc       0.75      0.62      0.68     33373\n",
      "    nh_black       0.68      0.31      0.42     28535\n",
      "    nh_white       0.80      0.94      0.87    134206\n",
      "\n",
      "    accuracy                           0.79    200000\n",
      "   macro avg       0.73      0.54      0.60    200000\n",
      "weighted avg       0.77      0.79      0.76    200000\n",
      "\n",
      "[[  1154    497    337   1898]\n",
      " [    71  20830    859  11613]\n",
      " [   160   1893   8840  17642]\n",
      " [   233   4660   3038 126275]]\n"
     ]
    }
   ],
   "source": [
    "p = model.predict(X_test, verbose=2) # to predict probability\n",
    "y_pred = np.argmax(p, axis=-1)\n",
    "target_names = list(sdf.race.astype('category').cat.categories)\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names))\n",
    "print(confusion_matrix(np.argmax(y_test, axis=1), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../dataverse_files/fl_voter_reg/lstm/fl_all_ln_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df = pd.DataFrame(words_list, columns=['vocab'])\n",
    "words_df.to_csv('../dataverse_files/fl_voter_reg/lstm/fl_all_ln_vocab.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainity Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build uncertainty model...\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 20, 32)            35296     \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 118,244\n",
      "Trainable params: 118,244\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build uncertainty model...')\n",
    "\n",
    "#model_uncrtn = Sequential()\n",
    "input_ = keras.layers.Input(shape=(feature_len))\n",
    "layer_ = Embedding(num_words, 32) (input_)\n",
    "layer_ = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(layer_)\n",
    "layer_ = Dropout(0.25)(layer_, training=True)\n",
    "output = Dense(num_classes, activation='softmax') (layer_)\n",
    "\n",
    "model_uncrtn = keras.models.Model(inputs=input_, outputs=output)\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model_uncrtn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_uncrtn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/15\n",
      "22500/22500 [==============================] - 431s 19ms/step - loss: 0.7522 - accuracy: 0.7338 - val_loss: 0.6808 - val_accuracy: 0.7638\n",
      "Epoch 2/15\n",
      "22500/22500 [==============================] - 415s 18ms/step - loss: 0.6772 - accuracy: 0.7651 - val_loss: 0.6616 - val_accuracy: 0.7712\n",
      "Epoch 3/15\n",
      "22500/22500 [==============================] - 408s 18ms/step - loss: 0.6619 - accuracy: 0.7707 - val_loss: 0.6491 - val_accuracy: 0.7765\n",
      "Epoch 4/15\n",
      "22500/22500 [==============================] - 407s 18ms/step - loss: 0.6499 - accuracy: 0.7757 - val_loss: 0.6447 - val_accuracy: 0.7782\n",
      "Epoch 5/15\n",
      "22500/22500 [==============================] - 417s 19ms/step - loss: 0.6472 - accuracy: 0.7769 - val_loss: 0.6423 - val_accuracy: 0.7796\n",
      "Epoch 6/15\n",
      "22500/22500 [==============================] - 418s 19ms/step - loss: 0.6399 - accuracy: 0.7793 - val_loss: 0.6395 - val_accuracy: 0.7812\n",
      "Epoch 7/15\n",
      "22500/22500 [==============================] - 413s 18ms/step - loss: 0.6399 - accuracy: 0.7797 - val_loss: 0.6380 - val_accuracy: 0.7812\n",
      "Epoch 8/15\n",
      "22500/22500 [==============================] - 409s 18ms/step - loss: 0.6369 - accuracy: 0.7804 - val_loss: 0.6383 - val_accuracy: 0.7810\n",
      "Epoch 9/15\n",
      "22500/22500 [==============================] - 408s 18ms/step - loss: 0.6373 - accuracy: 0.7800 - val_loss: 0.6347 - val_accuracy: 0.7822\n",
      "Epoch 10/15\n",
      "22500/22500 [==============================] - 416s 18ms/step - loss: 0.6350 - accuracy: 0.7812 - val_loss: 0.6347 - val_accuracy: 0.7826\n",
      "Epoch 11/15\n",
      "22500/22500 [==============================] - 411s 18ms/step - loss: 0.6341 - accuracy: 0.7818 - val_loss: 0.6332 - val_accuracy: 0.7830\n",
      "Epoch 12/15\n",
      "22500/22500 [==============================] - 405s 18ms/step - loss: 0.6329 - accuracy: 0.7823 - val_loss: 0.6337 - val_accuracy: 0.7826\n",
      "Epoch 13/15\n",
      "22500/22500 [==============================] - 404s 18ms/step - loss: 0.6327 - accuracy: 0.7822 - val_loss: 0.6339 - val_accuracy: 0.7839\n",
      "Epoch 14/15\n",
      "22500/22500 [==============================] - 405s 18ms/step - loss: 0.6305 - accuracy: 0.7827 - val_loss: 0.6316 - val_accuracy: 0.7845\n",
      "Epoch 15/15\n",
      "22500/22500 [==============================] - 405s 18ms/step - loss: 0.6338 - accuracy: 0.7821 - val_loss: 0.6337 - val_accuracy: 0.7841\n",
      "6250/6250 [==============================] - 25s 4ms/step - loss: 0.6337 - accuracy: 0.7835\n",
      "Test score: 0.6337046027183533\n",
      "Test accuracy: 0.7835050225257874\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model_uncrtn.fit(X_train, y_train, batch_size=batch_size, epochs=EPOCHS,\n",
    "          validation_split=0.1, verbose=1)\n",
    "score, acc = model_uncrtn.evaluate(X_test, y_test,\n",
    "                            batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 23s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 21s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 23s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 22s\n",
      "6250/6250 - 23s\n",
      "6250/6250 - 24s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 28s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n",
      "6250/6250 - 27s\n",
      "6250/6250 - 26s\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "ITER=100\n",
    "\n",
    "for _ in range(ITER):\n",
    "    predictions.append(model_uncrtn.predict(X_test, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = pd.DataFrame()\n",
    "\n",
    "predict_array = np.array(predictions)\n",
    "predict_df['mean'] = predict_array.mean(axis=0).reshape(-1,)\n",
    "predict_df['std'] = predict_array.std(axis=0).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015331</td>\n",
       "      <td>0.003421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153913</td>\n",
       "      <td>0.028474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.110329</td>\n",
       "      <td>0.024676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.720427</td>\n",
       "      <td>0.042752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006464</td>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.052338</td>\n",
       "      <td>0.016314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.131506</td>\n",
       "      <td>0.020988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.809692</td>\n",
       "      <td>0.028678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.030162</td>\n",
       "      <td>0.005773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.421811</td>\n",
       "      <td>0.048279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean       std\n",
       "0  0.015331  0.003421\n",
       "1  0.153913  0.028474\n",
       "2  0.110329  0.024676\n",
       "3  0.720427  0.042752\n",
       "4  0.006464  0.002045\n",
       "5  0.052338  0.016314\n",
       "6  0.131506  0.020988\n",
       "7  0.809692  0.028678\n",
       "8  0.030162  0.005773\n",
       "9  0.421811  0.048279"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
